Patchset improving Linux kernel performance while low on memory
mm: preserves portion of mmap pages from being evicted while low on memory
 - Improves performance under heavy memory pressure, reduces stalls
 - Fully configurable via sysctl (vm.mmap_preserve_ratio, default = 10%)

More information can be found here: https://github.com/LekKit/linux-mmap-preserve

diff -Naurp a/include/linux/swap.h b/include/linux/swap.h
--- a/include/linux/swap.h	2022-03-16 16:49:02.000000000 +0500
+++ b/include/linux/swap.h	2022-08-30 12:21:34.253942593 +0500
@@ -328,6 +328,7 @@ extern unsigned long mem_cgroup_shrink_n
 						unsigned long *nr_scanned);
 extern unsigned long shrink_all_memory(unsigned long nr_pages);
 extern int vm_swappiness;
+extern int vm_mmap_preserve_ratio;
 extern int remove_mapping(struct address_space *mapping, struct page *page);
 extern unsigned long vm_total_pages;
 
diff -Naurp a/kernel/sysctl.c b/kernel/sysctl.c
--- a/kernel/sysctl.c	2022-03-16 16:49:02.000000000 +0500
+++ b/kernel/sysctl.c	2022-08-30 12:21:09.451049359 +0500
@@ -1377,6 +1377,15 @@ static struct ctl_table vm_table[] = {
 		.extra1		= &zero,
 		.extra2		= &one_hundred,
 	},
+	{
+		.procname       = "mmap_preserve_ratio",
+		.data           = &vm_mmap_preserve_ratio,
+		.maxlen         = sizeof(vm_mmap_preserve_ratio),
+		.mode           = 0644,
+		.proc_handler   = proc_dointvec_minmax,
+		.extra1         = &zero,
+		.extra2         = &one_hundred,
+	},
 #ifdef CONFIG_HUGETLB_PAGE
 	{
 		.procname	= "nr_hugepages",
diff -Naurp a/mm/vmscan.c b/mm/vmscan.c
--- a/mm/vmscan.c	2022-03-16 16:49:02.000000000 +0500
+++ b/mm/vmscan.c	2022-08-30 12:29:47.144580856 +0500
@@ -142,6 +142,12 @@ struct scan_control {
  * From 0 .. 100.  Higher means more swappy.
  */
 int vm_swappiness = 60;
+
+/*
+ * Percentage of non-evictable mmaped pages relative to total memory
+ */
+int vm_mmap_preserve_ratio = 10;
+
 /*
  * The total number of pages which are beyond the high watermark within all
  * zones.
@@ -2325,6 +2331,24 @@ out:
 			 */
 			some_scanned |= !!scan;
 		}
+
+		if (vm_mmap_preserve_ratio && nr[LRU_ACTIVE_FILE]) {
+			unsigned long active_file_pages;
+			unsigned long preserved_pages;
+			unsigned long excess_pages;
+
+			/*
+			 * Prevent scanning up to N% of file-backed pages in memory.
+			 * E.q. with vm_mmap_preserve_ratio = 10 on 10G system this means
+			 * at most 1G non-evictable mmaped pages
+			 */
+			active_file_pages = global_node_page_state(NR_ACTIVE_FILE);
+			preserved_pages = totalram_pages / 100 * vm_mmap_preserve_ratio;
+			excess_pages = (active_file_pages > preserved_pages)
+				? (active_file_pages - preserved_pages) : 0;
+
+			nr[LRU_ACTIVE_FILE] = min(excess_pages, nr[LRU_ACTIVE_FILE]);
+		}
 	}
 }
 
