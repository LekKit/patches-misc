Patchset improving Linux kernel performance while low on memory 
mm: preserves portion of mmap pages from being evicted while low on memory
 - Improves performance under heavy memory pressure, reduces stalls
 - Fully configurable via sysctl (vm.mmap_preserve_ratio, default = 10%)

More information can be found here: https://github.com/LekKit/linux-mmap-preserve

diff -Naurp a/include/linux/swap.h b/include/linux/swap.h
--- a/include/linux/swap.h	2022-08-25 14:18:40.000000000 +0500
+++ b/include/linux/swap.h	2022-08-28 23:59:51.659208216 +0500
@@ -362,6 +362,7 @@ extern unsigned long mem_cgroup_shrink_n
 						unsigned long *nr_scanned);
 extern unsigned long shrink_all_memory(unsigned long nr_pages);
 extern int vm_swappiness;
+extern int vm_mmap_preserve_ratio;
 extern int remove_mapping(struct address_space *mapping, struct page *page);
 extern unsigned long vm_total_pages;
 
diff -Naurp a/kernel/sysctl.c b/kernel/sysctl.c
--- a/kernel/sysctl.c	2022-08-25 14:18:40.000000000 +0500
+++ b/kernel/sysctl.c	2022-08-29 00:31:30.371904625 +0500
@@ -1443,6 +1443,15 @@ static struct ctl_table vm_table[] = {
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= &one_hundred,
 	},
+	{
+		.procname	= "mmap_preserve_ratio",
+		.data		= &vm_mmap_preserve_ratio,
+		.maxlen		= sizeof(vm_mmap_preserve_ratio),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= &one_hundred,
+	},
 #ifdef CONFIG_NUMA
 	{
 		.procname	= "numa_stat",
diff -Naurp a/mm/vmscan.c b/mm/vmscan.c
--- a/mm/vmscan.c	2022-08-25 14:18:40.000000000 +0500
+++ b/mm/vmscan.c	2022-08-29 00:02:43.085192606 +0500
@@ -168,6 +168,12 @@ struct scan_control {
  * From 0 .. 100.  Higher means more swappy.
  */
 int vm_swappiness = 60;
+
+/*
+ * Percentage of non-evictable mmaped pages relative to total memory
+ */
+int vm_mmap_preserve_ratio = 10;
+
 /*
  * The total number of pages which are beyond the high watermark within all
  * zones.
@@ -2567,6 +2573,24 @@ out:
 		*lru_pages += lruvec_size;
 		nr[lru] = scan;
 	}
+
+	if (vm_mmap_preserve_ratio && nr[LRU_ACTIVE_FILE]) {
+		unsigned long active_file_pages;
+		unsigned long preserved_pages;
+		unsigned long excess_pages;
+
+		/*
+		 * Prevent scanning up to N% of file-backed pages in memory.
+		 * E.q. with vm_mmap_preserve_ratio = 10 on 10G system this means
+		 * at most 1G non-evictable mmaped pages
+		 */
+		active_file_pages = global_node_page_state(NR_ACTIVE_FILE);
+		preserved_pages = totalram_pages() / 100 * vm_mmap_preserve_ratio;
+		excess_pages = (active_file_pages > preserved_pages)
+			? (active_file_pages - preserved_pages) : 0;
+
+		nr[LRU_ACTIVE_FILE] = min(excess_pages, nr[LRU_ACTIVE_FILE]);
+	}
 }
 
 /*
